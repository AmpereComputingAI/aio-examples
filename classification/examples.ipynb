{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Examples\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"../utils/1ampere_logo_Â®_primary_stacked_rgb.png\" width=\"50%\"/>    \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Ampere AI software stack is the software acceleration layer of Ampere Cloud Native Processors specifically dedicated to accelerating AI workloads running on Ampere Processors. Ampere Optimized AI Frameworks include PyTorch, TensorFlow, and ONNXRuntime. This drop-in library seamlessly supports all AI applications developed in the most popular AI frameworks. It works  right out-of-the-box without API changes or any additional coding. Additionally, the Ampere AI software engineering team provides the publicly accessbile Ampere Model Library (AML) for testing and benchmarking the performance ofAmpere Cloud Native Processors for some of the most common AI inference workloads.\n",
    "\n",
    "Please visit us at https://amperecomputing.com\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## ImageNet Dataset Overview\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "    <img align=\"left\" src=\"https://www.image-net.org/static_files/index_files/logo.jpg\" alt=\"nn\" style=\"width: 200px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "These examples are using subset of ImageNet classification validation set from year 2012.\n",
    "ImageNet is a large-scale classification dataset that has been instrumental in advancing computer vision and deep learning research.\n",
    "\n",
    "More info can be found here: https://image-net.org/\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.imagenet import ImageNet\n",
    "import utils.post_processing as pp\n",
    "import utils.benchmark as bench_utils\n",
    "\n",
    "LAT_BATCH_SIZE = 1\n",
    "THROUGHPUT_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency with ResNet-50 v1.5 in fp32 precision\n",
    "\n",
    "AIO offers a significant speed-up in standard fp32 inference scenarios. AIO exposes\n",
    "AIO API to control behavior of the optimizer.\n",
    "This example shows the performance of ResNet-50 v1.5 model in fp32 precision.\n",
    "Original ResNet paper can be found here: https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224)\n",
    "path_to_fp32_model = \"resnet_50_v15/resnet_50_v15_tf_fp32.pb\"\n",
    "path_to_fp16_model = \"resnet_50_v15/resnet_50_v15_tf_fp16.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first let's load the model\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_fp32_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! AIO_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# creating TF config\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.intra_op_parallelism_threads = bench_utils.get_intra_op_parallelism_threads()\n",
    "config.inter_op_parallelism_threads = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# preparing input and output dictionaries\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {\"softmax_tensor:0\": graph.get_tensor_by_name(\"softmax_tensor:0\")}\n",
    "\n",
    "# initialization of ImageNet dataset\n",
    "imagenet = ImageNet(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"RGB\",\n",
    "    pre_processing=\"VGG\",\n",
    "    is1001classes=True\n",
    ")\n",
    "\n",
    "input_array = imagenet.get_input_array(target_shape=input_shape)\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"input_tensor:0\"): input_array}\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(imagenet.path_to_latest_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# running the model with AIO enabled in fp32 precision\n",
    "\n",
    "tf.AIO.force_enable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_aio = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nResNet-50 v1.5 FP32 latency with AIO: {:.0f} ms\\n\".format(latency_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# running the model with AIO disabled in fp32 precision\n",
    "\n",
    "tf.AIO.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_no_aio = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nResNet-50 v1.5 FP32 latency without AIO: {:.0f} ms\\n\".format(latency_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# show the image\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"ResNet-50 v1.5 FP32 predictions with AIO enabled:\\n\")\n",
    "print(f\"Top-1 prediction: {pp.get_imagenet_names(imagenet.extract_top1(output_aio[0]) + 1)}\")\n",
    "print(f\"Top-5 predictions: {pp.get_imagenet_names(imagenet.extract_top5(output_aio[0]) + 1)}\")\n",
    "\n",
    "print(\"\\nResNet-50 v1.5 FP32 predictions with AIO disabled:\\n\")\n",
    "print(f\"Top-1 prediction: {pp.get_imagenet_names(imagenet.extract_top1(output_no_aio[0]) + 1)}\")\n",
    "print(f\"Top-5 predictions: {pp.get_imagenet_names(imagenet.extract_top5(output_no_aio[0]) + 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput (BS=32) with ResNet-50 v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's fill array of shape [32, 224, 224, 3] with our image\n",
    "\n",
    "input_array_bs32 = np.empty([THROUGHPUT_BATCH_SIZE, *input_shape, 3])  # NHWC order\n",
    "for i in range(THROUGHPUT_BATCH_SIZE):\n",
    "    input_array_bs32[i] = input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's start with fp32 non-aio case\n",
    "\n",
    "feed_dict = {graph.get_tensor_by_name(\"input_tensor:0\"): input_array_bs32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# running the model with AIO disabled in fp32 precision\n",
    "\n",
    "tf.AIO.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    _ = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "throughput_no_aio = THROUGHPUT_BATCH_SIZE / (finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we have to load fp16 variant now\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_fp16_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# preparing input and output dictionaries\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {\"softmax_tensor:0\": graph.get_tensor_by_name(\"softmax_tensor:0\")}\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"input_tensor:0\"): input_array_bs32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# running the model with AIO enabled in fp16 precision\n",
    "\n",
    "tf.AIO.force_enable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    _ = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "throughput_aio = THROUGHPUT_BATCH_SIZE / (finish - start)\n",
    "print(\"\\nResNet-50 v1.5 FP32 throughput without AIO: {:.0f} fps\".format(throughput_no_aio))\n",
    "print(\"\\nResNet-50 v1.5 FP16 throughput with AIO: {:.0f} fps\".format(throughput_aio))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
